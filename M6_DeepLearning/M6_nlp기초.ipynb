{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트의 토큰화\n",
        "**자연어 처리(NLP)에서 토큰화(Tokenization)**\n",
        "- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n",
        "- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n",
        "\n",
        "토큰화의 목적\n",
        "- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다.\n",
        "\n",
        "토큰화의 유형\n",
        "- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 NLP 작업의 요구 사항에 따라 선택됩니다:\n",
        "  - 단어 토큰화 (Word Tokenization):\n",
        "    텍스트를 개별 단어로 분해합니다.\n",
        "    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n",
        "  - 문장 토큰화 (Sentence Tokenization):\n",
        "    텍스트를 개별 문장으로 분해합니다.\n",
        "    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n",
        "  - 하위 단어 토큰화 (Subword Tokenization):\n",
        "    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n",
        "    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n",
        "  - 문자 토큰화 (Character Tokenization):\n",
        "    텍스트를 개별 문자로 분해합니다.\n",
        "    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
        "\n",
        "토큰화의 과정\n",
        "- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n",
        "  - 텍스트 정규화 (Text Normalization):\n",
        "    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n",
        "    불필요한 구두점과 공백을 제거합니다.\n",
        "    예: \"Hello, World!\" → \"hello world\"\n",
        "  - 구분자 사용 (Delimiter-based Tokenization):\n",
        "    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n",
        "    예: \"hello world\" → [\"hello\", \"world\"]\n",
        "  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n",
        "    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n",
        "    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n",
        "\n",
        "토큰화의 중요성\n",
        "- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 후속 처리와 분석의 정확도에 큰 영향을 미칠 수 있습니다.\n",
        "- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."
      ],
      "metadata": {
        "id": "sVLoF37P8XOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 고급 토큰화 기법\n",
        "언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n",
        "\n",
        "\n",
        "형태소 분석 (Morphological Analysis):\n",
        "- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n",
        "- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n",
        "- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n",
        "\n",
        "어간 추출 (Stemming):\n",
        "- 단어의 어간을 추출하여 형태를 단순화합니다.\n",
        "- 예: \"running\", \"runs\", \"ran\" → \"run\"\n",
        "- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n",
        "\n",
        "어근 추출 (Lemmatization):\n",
        "- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n",
        "- 예: \"running\", \"ran\" → \"run\"\n",
        "- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n",
        "- WordNetLemmatizer와 같은 도구가 사용됩니다.\n",
        "\n",
        "BPE (Byte Pair Encoding):\n",
        "- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n",
        "- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n",
        "- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n",
        "\n",
        "WordPiece:\n",
        "- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n",
        "- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n",
        "- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n",
        "\n",
        "SentencePiece:\n",
        "- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n",
        "- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n",
        "- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n",
        "- Google의 T5, ALBERT 모델에서 사용됩니다."
      ],
      "metadata": {
        "id": "pNzNZ7r698q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMcTKq_48Qry",
        "outputId": "7619ff43-5fb0-44f6-c285-30fc9d551bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "원문:\n",
            " 해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해보지', '않으면', '해낼', '수', '없다']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array\n",
        "\n",
        "# 케라스의 텍스트 전처리와 관련한 함수중 text_to_sequence 함수를 불러옴\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# 전처리할 텍스트를 정함\n",
        "text = '해보지 않으면 해낼 수 없다'\n",
        "\n",
        "# 해당 텍스트를 토큰화\n",
        "result = text_to_word_sequence(text)\n",
        "print('\\n원문:\\n', text)\n",
        "print('\\n토큰화:\\n', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n",
        "- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n",
        "- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n",
        "- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n",
        "- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n",
        "- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"
      ],
      "metadata": {
        "id": "GTo751ReAMD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도수 세기\n",
        "\n",
        "# 전처리 하려는 세 개의 문장을 정함\n",
        "\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
        "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n",
        "       '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.',\n",
        "       ]\n",
        "\n",
        "# 토큰화 함수를 이용해 전처리 하는 과정\n",
        "\n",
        "token = Tokenizer()             # 토큰화 함수 지정\n",
        "token.fit_on_texts(docs)        # 토큰화 함수에 문장 적용\n",
        "\n",
        "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n",
        "# Tokenizer()의 word_counts 함수는 순서를 기억하는 OderedDict 클래스를 사용\n",
        "print('\\n단어 카운트:\\n', token.word_counts)\n",
        "print('\\n문장 카운트:', token.document_count)\n",
        "print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n",
        "# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n",
        "# 동일한 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 할당\n",
        "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQISQPdgAPJ3",
        "outputId": "a6b1cf29-84f2-4d64-ed7b-841989548ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트: 3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'합니다': 1, '먼저': 1, '토큰화': 3, '각': 1, '나누어': 1, '텍스트의': 2, '단어를': 1, '딥러닝에서': 2, '해야': 1, '인식됩니다': 1, '단어로': 1, '한': 1, '있습니다': 1, '결과는': 1, '수': 1, '사용할': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 주어진 docs를 토큰화해서 아래사항을 수정"
      ],
      "metadata": {
        "id": "8CrKazPDC9mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n",
        "'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "\n",
        "print('\\n단어 카운트:\\n', token.word_counts)\n",
        "print('\\n문장 카운트:', token.document_count)\n",
        "print('\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n', token.word_docs)\n",
        "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KNOFMavDHEK",
        "outputId": "7571979b-4334-4100-e7af-c3c42479abc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n",
            "\n",
            "문장 카운트: 2\n",
            "\n",
            "각 단어가 몇개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'판단하면서': 1, '이후': 1, '사실': 1, '제시한': 1, '3년': 1, '재판부가': 1, '이': 1, '덜게': 1, '됐다': 1, '5개월여': 1, '혐의': 1, '무죄로': 1, '검찰': 2, '만에': 1, '시름을': 1, '전부를': 1, '회장은': 1, '검찰이': 1, '기소': 1, '진행될': 1, '나오지만': 1, '항소로': 1, '재판이': 1, '것이란': 1, '전망이': 1, '2심': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n",
        "\n",
        "Tokenizer를 사용한 텍스트 토큰화\n",
        "\n",
        "word_index:\n",
        "- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n",
        "- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n",
        "\n",
        "word_size:\n",
        "- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n",
        "- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n",
        "\n",
        "One-Hot Encoding:\n",
        "- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n",
        "- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n",
        "- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n",
        "\n",
        "num_classes:\n",
        "- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."
      ],
      "metadata": {
        "id": "BVknEowvF4s5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 레이어\n",
        "- 자연어 처리나 시퀀스 데이터를 다루는 모델에서 자주 사용되는 레이어로, 입력 데이터를 고정된 크기의 고차원 벡터로 변환해주는 역할을 합니다.\n",
        "\n",
        "주요 기능:\n",
        "  - 단어를 벡터로 변환: Embedding 레이어는 고유한 정수 인덱스로 표현된 단어들을 특정 크기의 벡터로 변환합니다. 이 벡터들은 학습 과정에서 모델이 조정할 수 있는 가중치로 초기화됩니다.\n",
        "  - 차원 축소 및 특성 학습: 높은 차원의 단어를 저차원의 벡터로 변환함으로써 데이터의 차원을 축소하고, 단어 간의 유사성을 학습할 수 있습니다. 예를 들어, 비슷한 의미의 단어들이 유사한 벡터로 매핑되도록 학습됩니다.\n",
        "\n",
        "입력 및 출력:\n",
        "  - 입력: 정수로 인코딩된 시퀀스 데이터. 예를 들어, 단어를 고유한 정수로 인코딩한 시퀀스.\n",
        "  - 출력: 각 입력 정수에 대응하는 고정된 크기의 밀집 벡터."
      ],
      "metadata": {
        "id": "AJdeys4BeXrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Input\n",
        "\n",
        "# 입력 시퀀스의 길이를 3이라고 가정\n",
        "input_data = tf.constant([1, 2, 3], dtype=tf.int32)\n",
        "\n",
        "# 임배딩 레이어 정의\n",
        "embedding_layer = Embedding(input_dim=10, output_dim=5)\n",
        "\n",
        "# 텐서로 전달해야 함 (아래와 같이 사용)\n",
        "output = embedding_layer(input_data)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef97ZBnNfKJB",
        "outputId": "8fe69e29-2849-46de-a45a-63dfd657977c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.01191896  0.01133019  0.01414179 -0.0444744   0.0256226 ]\n",
            " [ 0.00321301 -0.01400135  0.002152    0.04735493  0.01616802]\n",
            " [-0.00868054 -0.01103047 -0.01851816  0.0043829   0.01378015]], shape=(3, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKqVRk4eEyF5",
        "outputId": "48596c04-84f3-4b3b-d47a-a00f7c82aa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqpjKBo3FR8S",
        "outputId": "31375b9f-2e0d-4c01-f2f5-93ad17e03c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
        "# word_size 에 1을 추가하는 이유는 keras의 tokenizer를 사용할 때 0 인덱스를 패딩(padding)을 위해 예약하는 관례 때문\n",
        "\n",
        "word_size = len(token.word_index) + 1\n",
        "x = to_categorical(x, num_classes=word_size)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj4JSv_YFZgH",
        "outputId": "e8274a5b-2bf8-47d9-c265-c9f510631d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array"
      ],
      "metadata": {
        "id": "uV82iwjwf5tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras Tokenizer의 fit_on_texts 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에 단일 문자열을 입력하는 것은 적절하지 않다\n",
        "text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azNEgHyHf97n",
        "outputId": "8079d229-8ae3-49e8-acf1-f0cdcae3bc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences([text])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CItsdkpgQSk",
        "outputId": "4cf944d5-66eb-49fb-e953-325535377587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
        "# word_size 에 1을 추가하는 이유는 keras의 Tokenizer를 사용할떄  0의 인덱스를 패딩(padding)을 위해 예약하는 관례 때문\n",
        "\n",
        "word_size = len(token.word_index) + 1\n",
        "x = to_categorical(x, num_classes=word_size)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_PFWdgcgWIY",
        "outputId": "02400133-d904-4240-f152-dc172c224d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[1. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[1. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[1. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[1. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[1. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 1. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]\n",
            "    [1. 0. 0. ... 0. 0. 0.]]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 리뷰 자료를 지정합니다\n",
        "\n",
        "docs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다','한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n",
        "\n",
        "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# 토큰화\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb8cocRrg99u",
        "outputId": "36c3bdb1-836f-4487-8bf6-bc53ac87e675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences(docs)\n",
        "print('\\n리뷰 텍스트, 토큰화 결과:\\n', x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b846fzkmh6HK",
        "outputId": "31544ec9-62b0-4dba-a252-ebc03b15a9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "리뷰 텍스트, 토큰화 결과:\n",
            " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줌\n",
        "# padding='pre' : 시퀀스의 길이가 4보다 짧은 경우 앞쪽을 0으로 채워 길이를 4로 맞춤\n",
        "\n",
        "padded_x = pad_sequences(x, 4, padding='pre') # default\n",
        "# padded_x = pad_sequences(x, 4, padding='post')\n",
        "print('\\n패딩 결과:\\n', padded_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Q-RFCAidRl",
        "outputId": "e6e53cc8-c9ed-4b75-e862-9d2608644094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "패딩 결과:\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "- 기계 학습, 특히 자연어 처리(NLP)의 맥락에서 임베딩은 단어, 구문 또는 기타 유형의 엔터티를 실수의 밀집된 벡터로 표현하는 기술을 의미. 핵심 아이디어는 이러한 엔터티의 의미론적 의미를 연속적인 벡터 공간으로 인코딩하는 것이다. 여기서 벡터 간의 기하학적 관계는 이들이 나타내는 엔터티 간의 의미론적 관계를 반영하며 이 접근 방식은 의미론적 관계가 캡처되지 않는 원-핫 인코딩과 같은 희소 표현과 대조된다.\n",
        "- 의미:\n",
        "  - 의미적 표현: 임베딩 벡터는 단어나 개체의 의미를 포착하도록 설계된다. 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까운 벡터를 갖도록 처리된다.\n",
        "  - 차원성 감소: 임베딩은 고차원 공간(예: 원-핫 인코딩된 벡터)의 단어를 저차원의 연속 벡터 공간으로 매핑하며 이는 표현을 더욱 효율적으로 만들고 계산 복잡성을 줄인다.\n",
        "  - 컨텍스트화: 고급 임베딩 모델(예: Word2Vec, GloVe, BERT)에서는 단어가 나타나는 컨텍스트가 벡터 표현에 영향을 미치므로 모델이 다양한 컨텍스트에서 단어의 다양한 의미를 캡처할 수 있다.\n",
        "- 임베딩 생성 방법:\n",
        "  - 사전 훈련된 임베딩: 일반적인 접근 방식 중 하나는 대규모 텍스트 모음에 대해 사전 훈련된 임베딩을 사용하는 것으로 Word2Vec, GloVe와 같은 모델을 사용하면 훈련 코퍼스에서 풍부한 의미 체계 관계를 학습한 사전 훈련된 모델을 기반으로 단어를 벡터에 매핑할 수 있다.\n",
        "    - Word2Vec: 컨텍스트를 사용하여 대상 단어를 예측하거나(CBOW) 단어를 사용하여 컨텍스트를 예측(Skip-gram)하여 임베딩을 학습한다.\n",
        "      - CBOW 모델은 주어진 컨텍스트(주변 단어들)를 바탕으로 타겟 단어를 예측하는 방식으로 작동. 예를 들어, 문장 \"the cat sits on the\"에서 \"cat\", \"sits\", \"on\", \"the\"를 컨텍스트로 사용하여 \"mat\"이라는 타겟 단어를 예측\n",
        "      - skip-gram 모델은 CBOW와 반대로, 주어진 타겟 단어로부터 컨텍스트(주변 단어들)를 예측하는 방식으로 작동. 예를 들어, \"cat\"이라는 단어가 주어졌을 때, \"the\", \"sits\", \"on\", \"the\"와 같은 주변 단어들을 예측\n",
        "    - GloVe: 단어 동시 발생 통계 행렬을 인수분해하여 임베딩을 학습한다.    \n",
        "  - 자신만의 임베딩 훈련: 신경망을 사용하여 처음부터 자신만의 임베딩을 훈련할 수도 있다."
      ],
      "metadata": {
        "id": "TpGPvS0lk8pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 입력을 표현하기 위해 단어 임베딩 활용\n",
        "- model.add(Embedding(word_size, 8, input_length=4)): 이 줄은 모델에 임베딩 레이어를 추가. 임베딩 레이어는 단어 임베딩을 만드는 데 사용. 각 매개변수의 의미는 다음과 같다.\n",
        "  - word_size: 입력 차원의 크기, 즉 어휘 크기. 데이터 세트에 있는 총 고유 단어 수에 1을 더한 값.\n",
        "  - 8: 임베딩 벡터의 크기. 각 단어는 8차원 벡터로 표현.\n",
        "  - input_length=4: 입력 시퀀스의 길이. 이 모델은 각 입력 시퀀스의 길이가 4일 것으로 예상(예: 입력당 4개의 단어).\n",
        "- model.add(Flatten()): 임베딩 레이어 이후 출력 모양은 배치 크기를 포함하여 3차원. 이 레이어는 출력을 2차원(배치 크기, input_length * 8)으로 평면화하여 밀도가 높은 레이어에 직접 공급될 수 있도록 한다."
      ],
      "metadata": {
        "id": "7qpbgHTpmavI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "docs = ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다','한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n",
        "\n",
        "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "Qi3Ld03ImuRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)\n",
        "\n",
        "# 임베딩에 입력될 단어의 수를 지정\n",
        "word_size = len(token.word_index) + 1\n",
        "\n",
        "# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(4,)))\n",
        "model.add(Embedding(word_size, 8))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "AGtK0KRnmyPs",
        "outputId": "cc3e2e72-0f7e-42c4-e59a-cf16458792e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)                │             \u001b[38;5;34m168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201\u001b[0m (804.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> (804.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201\u001b[0m (804.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> (804.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token.texts_to_sequences(docs)\n",
        "padded_x = pad_sequences(x, 4, padding='pre')"
      ],
      "metadata": {
        "id": "aKu7Z9Fmrkat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(padded_x, classes, epochs=20)\n",
        "print('\\n Accuracy: %.4f' % (model.evaluate(padded_x, classes)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nruCBGx2sL6f",
        "outputId": "b41eabf4-91a8-4dbd-b5ed-433586c52ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957ms/step - accuracy: 0.5000 - loss: 0.6920\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6902\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6883\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6000 - loss: 0.6865\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8000 - loss: 0.6847\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8000 - loss: 0.6829\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.6811\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.6793\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.6775\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.6757\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9000 - loss: 0.6739\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6721\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6703\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6685\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6667\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6649\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6631\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6613\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6595\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.6577\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 1.0000 - loss: 0.6559\n",
            "\n",
            " Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. 10개의 단어를 가진 어휘(vocabulary)와 각 단어를 4차원 벡터로 임베딩하는 Keras 모델을 구성하세요. 시퀀스의 최대 길이는 5로 설정하세요.\n",
        "\n",
        "`input_data = np.array([\n",
        "    [1, 2, 3, 4, 5],   \n",
        "    [2, 3, 4, 5, 6]    \n",
        "])`"
      ],
      "metadata": {
        "id": "TVbT-hKVtIrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "input_data = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [2, 3, 4, 5, 6]\n",
        "])\n",
        "\n",
        "# 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(5,))) # Input 레이어 추가\n",
        "model.add(Embedding(input_dim=10, output_dim=4)) # Input_lengh 제거\n",
        "\n",
        "# Flatten 레이어 추가 (임베딩 출력의 차원을 평평하게 하기 위해)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense 레이어 추가 (출력을 보기 위해 간단한 밀집 레이어)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "6rWKg0vVs6vm",
        "outputId": "a2383582-c820-4afb-ced1-82a0a5619c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m4\u001b[0m)                │              \u001b[38;5;34m40\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q. IMDB 영화 리뷰 데이터셋을 사용하여 긍부정 이진부류 모델링 및 평가를 수행. 단, embedding 차원은 8"
      ],
      "metadata": {
        "id": "bXhj5oAGwGEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# 데이터셋 로드\n",
        "# num_words=1000은 훈련 데이터에서 가장 자주 나타느는 상위 10,000개의 단어만 사용하겠다는 의미\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# 시퀀스 데이터 패딩\n",
        "x_train = pad_sequences(x_train, maxlen=100)\n",
        "x_test = pad_sequences(x_test, maxlen=100)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(100,)))\n",
        "model.add(Embedding(10000, 8)) # 입력 차원은 단어 인덱스의 최대 값 + 1, 출력 차원은 임배딩 후 벡터 크기\n",
        "model.add(Flatten())     # 임베딩된 시퀀스를 평탄화\n",
        "model.add(Dense(1, activation='sigmoid'))    # 긍정과 부정을 분류하므로 시그모이드 활성화 함수 사용\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy:, {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "Vwxm4iF0wa-n",
        "outputId": "13526217-dc5f-4557-cf98-f9ebd5c5ba32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │          \u001b[38;5;34m80,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m801\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5999 - loss: 0.6603 - val_accuracy: 0.8250 - val_loss: 0.4047\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8657 - loss: 0.3390 - val_accuracy: 0.8448 - val_loss: 0.3399\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9122 - loss: 0.2374 - val_accuracy: 0.8498 - val_loss: 0.3365\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9376 - loss: 0.1842 - val_accuracy: 0.8488 - val_loss: 0.3397\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9607 - loss: 0.1391 - val_accuracy: 0.8476 - val_loss: 0.3535\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9752 - loss: 0.1070 - val_accuracy: 0.8394 - val_loss: 0.3728\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0748 - val_accuracy: 0.8396 - val_loss: 0.3964\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0533 - val_accuracy: 0.8366 - val_loss: 0.4193\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0338 - val_accuracy: 0.8326 - val_loss: 0.4505\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0243 - val_accuracy: 0.8336 - val_loss: 0.4763\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8352 - loss: 0.4923\n",
            "Test Accuracy:, 0.8343999981880188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 예측할 새로운 텍스트 데이터\n",
        "new_reviews = [\n",
        "    \"This movie was fantastic! I really enjoyed it.\",\n",
        "    \"Terrible movie, I will never watch it again.\",\n",
        "    \"It was an average film, nothing special.\",\n",
        "    \"Absolutely loved the movie, great acting and story!\",\n",
        "    \"Worst movie ever, completely wasted my time.\"\n",
        "]\n",
        "\n",
        "# 이미 학습된 토크나이저로 시퀀스를 변환\n",
        "word_index = imdb.get_word_index() # 함수는 IMDB 데이터셋에 사용된 단어와 그 단어에 대응하는 인덱스를 매핑한 딕셔너리를 반환\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "# v + 3은 원래 IMDB 단어 인덱스에서 사용된 정수 인덱스 값에 3을 더한 새로운 인덱스를 생성\n",
        "tokenizer.word_index = {k: (v + 3) for k, v in word_index.items()} # 특정 예약된 인덱스(예: <PAD>, <START>, <UNK> 등)를 위해 앞부분의 인덱스를 비워두기 위함\n",
        "tokenizer.word_index[\"<PAD>\"] = 0 # 패딩 토큰을 나타내며, 인덱스 0을 부여\n",
        "tokenizer.word_index[\"<START>\"] = 1 # 시퀀스의 시작을 나타내는 토큰이며, 인덱스 1을 부여\n",
        "tokenizer.word_index[\"<UNK>\"] = 2 # 인덱스에 포함되지 않은 단어들을 대체하기 위해 사용됩니다. 인덱스 2를 부여\n",
        "tokenizer.word_index[\"<UNUSED>\"] = 3 # 사용되지 않는 토큰을 위해 인덱스 3을 부여\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(new_reviews)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(padded_sequences)\n",
        "\n",
        "# 예측 결과 출력\n",
        "for i, review in enumerate(new_reviews):\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Predicted Sentiment: {'Positive' if predictions[i] > 0.5 else 'Negative'}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bktXQ9C47DVb",
        "outputId": "124b7055-31d0-4f32-ee58-b9e66895233c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Review: This movie was fantastic! I really enjoyed it.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Terrible movie, I will never watch it again.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: It was an average film, nothing special.\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Absolutely loved the movie, great acting and story!\n",
            "Predicted Sentiment: Positive\n",
            "\n",
            "Review: Worst movie ever, completely wasted my time.\n",
            "Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}